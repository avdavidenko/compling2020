{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/oserikov/f430e81939ffff48cafd6377b9e67b9c/.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh6wr5m8ZFot"
   },
   "source": [
    "# Домашнее задание о векторизации текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bm1AFbqZFIY"
   },
   "source": [
    "В задании вам предстоит сравнить несколько методов снижения размерности\n",
    "* PCA\n",
    "* t-SNE\n",
    "\n",
    "а так же попробовать осуществить тематическое моделирование методом LDA.\n",
    "\n",
    "**Формат сдачи задания** -- указание в гуглформе ссылки на тетрадь с решением + ответ на вопросы (см. последние вопросы первой задачи) в форме. Форма появится ближе к дедлайну.\n",
    "\n",
    "**Дедлайн** 23.59 7 октября MSK.  \n",
    "\n",
    "ДЗ предполагает возможность получения **до 12 баллов** по десятибалльной шкале. Оценки 11 и 12 поступают в ведомость, как оценки 11 и 12.\n",
    "\n",
    "---\n",
    "\n",
    "Если вы уже хорошо знакомы с снижением размерности, реализуйте первую задачу, используя не Bag-of-Words векторы текстов, а эмбеддинги текстов, полученные алгоритмом на ваш выбор. \n",
    "**Если вы собираетесь решать задачу так, то, приступая, сообщите об этом @oserikov в телеграме.**\n",
    "\n",
    "Если вы уже хорошо знакомы ещё и с векторизацией текстов эмбеддингами, напишите @oserikov для обсуждения замены первой задачи на другую.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHqgPgL_Z9XF"
   },
   "source": [
    "# [6 баллов] Задача о снижении размерности\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYhZCZwHnSan"
   },
   "source": [
    "[Вот](https://drive.google.com/drive/folders/1HX5rz4UZHtbzhPguUFolOg-xm6HFc0KO?usp=sharing) корпус, однажды собранный без особенных размышлений.\n",
    "Это -- корпус любительской литературы. Он был собран для забавы и непонятно, какая природа у представленных там текстов.\n",
    "\n",
    "Вам предстоит оценить, насколько эти тексты интересны в качестве простого датасета для задачи классификации: информативны ли Bag-of-Words векторы в смысле разделения текстов по жанрам.\n",
    "\n",
    "---\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "**Задача**: взяв фанифики и два каких-то других жанра из корпуса, визуализировать их BoW-представления на плоскости.\n",
    "\n",
    "---\n",
    "\n",
    "Визуализацию стоит осуществлять scatter-плотом, информацию о принадлежности документа какому-то жанру стоит передавать цветом.\n",
    "\n",
    "Количество документов, представляющих каждый жанр, стоит подобрать семплированием нужного количества элементов под доступные вычислительные ресурсы -- полный корпус точно слишком велик.\n",
    "\n",
    "Гиперпараметры BoW-векторизатора стоит подобрать под доступные вычислительные ресурсы -- если код работает дольше часа, то стоит упростить вычислительную задачу: подобрать другие гиперпараметры векторизации или уменьшить выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIbAkSSnnOp8"
   },
   "source": [
    "#### Критерии\n",
    "\n",
    "\n",
    "* **1 БАЛЛ**: \n",
    "  * В выбранных документах осуществлена какая-то **стандартная предобработка текста**: удалены стоп-слова и мусорные токены (e.g. html-теги), проведена лемматизация.  \n",
    "  Решение о каждой конкретной детали предобработки остаётся на усмотрение студентов: каждое нестандартное действие (e.g. отказ от лемматизации или удаление каких-то особенных токенов) стоит пояснить коротким комментарием, описывающим мотивацию.\n",
    "  * Получены **Bag-of-Words векторы** документов, выбранных для исследования. \n",
    "* **1 БАЛЛ**: получена визуализация документов на плоскости **методом главных компонент** снижения размерности Bag-of-Words векторов.\n",
    "* **1 БАЛЛ**: получена визуализация документов на плоскости методом **t-SNE** снижения размерности Bag-of-Words векторов.\n",
    "* **1 БАЛЛ**: на полученных визуализациях **получилось передать цветом точек классы** документов; понятно, точка какого цвета относится к какому классу.\n",
    "\n",
    "\n",
    "Скорее всего визуализация t-SNE и PCA заметно отличаются раскладкой точек по плоскости: один метод как будто раскладывает их вдоль двух пересекающихся прямых, за другим такого свойства скорее всего нет. Ответ на два вопроса ниже вам предстоит указать в гуглформе, сдавая задание.\n",
    "* **1 БАЛЛ**: верно указано, какой метод укладывает точки примерно вдоль прямых, а какой -- нет\n",
    "* **1 БАЛЛ**: предложено верное описание тому, почему у одного из методов всегда результаты располагаются вдоль некоторых прямых. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMEZ9Qng58B5"
   },
   "source": [
    "#### Примеры кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzf0J9IX6Kr6"
   },
   "source": [
    "Использование t-SNE и PCA для визуализации векторов: [ссылка](https://www.kaggle.com/jbencina/clustering-documents-with-tfidf-and-kmeans).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стандартная предобработка текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize (text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[.!?]\\s', r' ', text)\n",
    "    words = text.split()\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            clean_words.append(word)\n",
    "    return clean_words\n",
    "\n",
    "def lemmatize (text):\n",
    "    words = tokenize(text)\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = morph.parse(word)[0].normal_form\n",
    "        lemmas.append(lemma)\n",
    "        new_lemmas = ' '.join(lemmas)        \n",
    "    return new_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "num_of_texts = 5\n",
    "\n",
    "folders = ['scifi', 'fanfiction', 'esoterics']\n",
    "names = []\n",
    "texts = []\n",
    "genres = []\n",
    "\n",
    "new_folders = []\n",
    "for folder in folders:\n",
    "    new_folders.append (os.walk(folder))\n",
    "\n",
    "for folder in new_folders:\n",
    "    for address, dirs, files in folder:\n",
    "        i = 0\n",
    "        for file in files:\n",
    "            names.append(file)\n",
    "            path = address+'/'+file\n",
    "            opened = open(path, 'r', encoding = 'utf-8')\n",
    "            text = opened.read()\n",
    "            texts.append(text)\n",
    "            genres.append(address)\n",
    "            i += 1\n",
    "            if i > num_of_texts - 1:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "for text in texts:\n",
    "    text = lemmatize(text)\n",
    "    cleaned.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['genre'] = genres\n",
    "df['name'] = names\n",
    "df['texts'] = texts\n",
    "df['lemmas'] = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>name</th>\n",
       "      <th>texts</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scifi</td>\n",
       "      <td>makar_dmyeshepowojuem.txt</td>\n",
       "      <td>\\nЧто-то стучится в висок. То ли кровь, то ли ...</td>\n",
       "      <td>что-то стучаться висок кровь, недавний выстрел...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scifi</td>\n",
       "      <td>maks_roudi.txt</td>\n",
       "      <td>\\n\\n\\n\\n   \\n\\n   МАКС  РОУД љ\\n\\n\\n   05.11.2...</td>\n",
       "      <td>макс роуд љ 05.11.2014 - 03.04.2015 агония маз...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scifi</td>\n",
       "      <td>mak_ivanzzz_ideal3.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n Ivan Mak\\n Идеальный мир\\n\\...</td>\n",
       "      <td>ivan mak идеальный мир предисловие медленно по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scifi</td>\n",
       "      <td>malaja_m_s16iacmoguch21.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\nГлава 15.\\n\\n***\\n\\n4403 цикл Космич...</td>\n",
       "      <td>глава 15 *** 4403 цикл космический эра планет ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scifi</td>\n",
       "      <td>malinowskaja_majja_igorewnaplennikiuest.txt</td>\n",
       "      <td>\\n\\n\\n \\n \\n\\n\\n\\n\\n\\nМалиновкая Майя\\n\\n\\n\\n\\...</td>\n",
       "      <td>малиновкий майя пленник уэст книга 2 фантастич...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fanfiction</td>\n",
       "      <td>kowalenko_e_blentochka2.txt</td>\n",
       "      <td>\\nЛенточка \\n\\n              Фанфик по роману ...</td>\n",
       "      <td>ленточка фанфик роман андрей круз \"земля лишни...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fanfiction</td>\n",
       "      <td>kowizhenko_w_wmasseffectwhilethereaperukr.txt</td>\n",
       "      <td>\\n\" Mass Effect : While the Reaper \"\\n\\nНазва:...</td>\n",
       "      <td>\" mass effect : while the reaper \" назва: \" ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fanfiction</td>\n",
       "      <td>kram_dhivepodkidyshiulxja.txt</td>\n",
       "      <td>\\n\\nДмитрий Крам.\\nS-T-I-K-S. Подкидыши Улья.\\...</td>\n",
       "      <td>дмитрий крам s-t-i-k-s подкидыш улья андрей пр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fanfiction</td>\n",
       "      <td>kram_discheznuwshijklan.txt</td>\n",
       "      <td>\\n\\n\\nДмитрий Крам.\\n\\nИсчезнувший клан. Фанфи...</td>\n",
       "      <td>дмитрий крам исчезнуть клан фанфик \"играть жит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fanfiction</td>\n",
       "      <td>krasnoperowa_ahp_peste.txt</td>\n",
       "      <td>\\n\\n\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\tСодержание:\\n\\t\\...</td>\n",
       "      <td>содержание: часть 1 акклиматиазия часть 2 полё...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>esoterics</td>\n",
       "      <td>mahinenko_w_w499ahonija.txt</td>\n",
       "      <td>\\n\\n      АГОНИЯ   МАСОНОВ\\n\\nДа вы поняли хот...</td>\n",
       "      <td>агония масон понять хоть, понатворить одессе, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>esoterics</td>\n",
       "      <td>majklow_wgjcjbt.txt</td>\n",
       "      <td>\\n\\nФрагмент. Книгу можно купить в таких магаз...</td>\n",
       "      <td>фрагмент книга купить такой магазинах, 'литрес...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>esoterics</td>\n",
       "      <td>majklow_wyoga.txt</td>\n",
       "      <td>\\n\\nФрагмент. Книгу можно купить в таких магаз...</td>\n",
       "      <td>фрагмент книга купить такой магазинах, 'литрес...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>esoterics</td>\n",
       "      <td>majorow_w_sdno_zemli6.txt</td>\n",
       "      <td>\\n\\n\\nОгненное солнце, зажженное демоном под к...</td>\n",
       "      <td>огненный солнце, зажечь демон купол пещеры, ме...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>esoterics</td>\n",
       "      <td>majorow_w_skardinal.txt</td>\n",
       "      <td>\\n \\n              Кардинал Луи Де Шеврон поль...</td>\n",
       "      <td>кардинал луи де шеврон пользоваться среди горо...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         genre                                           name  \\\n",
       "0        scifi                      makar_dmyeshepowojuem.txt   \n",
       "1        scifi                                 maks_roudi.txt   \n",
       "2        scifi                         mak_ivanzzz_ideal3.txt   \n",
       "3        scifi                    malaja_m_s16iacmoguch21.txt   \n",
       "4        scifi    malinowskaja_majja_igorewnaplennikiuest.txt   \n",
       "5   fanfiction                    kowalenko_e_blentochka2.txt   \n",
       "6   fanfiction  kowizhenko_w_wmasseffectwhilethereaperukr.txt   \n",
       "7   fanfiction                  kram_dhivepodkidyshiulxja.txt   \n",
       "8   fanfiction                    kram_discheznuwshijklan.txt   \n",
       "9   fanfiction                     krasnoperowa_ahp_peste.txt   \n",
       "10   esoterics                    mahinenko_w_w499ahonija.txt   \n",
       "11   esoterics                            majklow_wgjcjbt.txt   \n",
       "12   esoterics                              majklow_wyoga.txt   \n",
       "13   esoterics                      majorow_w_sdno_zemli6.txt   \n",
       "14   esoterics                        majorow_w_skardinal.txt   \n",
       "\n",
       "                                                texts  \\\n",
       "0   \\nЧто-то стучится в висок. То ли кровь, то ли ...   \n",
       "1   \\n\\n\\n\\n   \\n\\n   МАКС  РОУД љ\\n\\n\\n   05.11.2...   \n",
       "2   \\n\\n\\n\\n\\n\\n\\n\\n\\n Ivan Mak\\n Идеальный мир\\n\\...   \n",
       "3   \\n\\n\\n\\n\\nГлава 15.\\n\\n***\\n\\n4403 цикл Космич...   \n",
       "4   \\n\\n\\n \\n \\n\\n\\n\\n\\n\\nМалиновкая Майя\\n\\n\\n\\n\\...   \n",
       "5   \\nЛенточка \\n\\n              Фанфик по роману ...   \n",
       "6   \\n\" Mass Effect : While the Reaper \"\\n\\nНазва:...   \n",
       "7   \\n\\nДмитрий Крам.\\nS-T-I-K-S. Подкидыши Улья.\\...   \n",
       "8   \\n\\n\\nДмитрий Крам.\\n\\nИсчезнувший клан. Фанфи...   \n",
       "9   \\n\\n\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\tСодержание:\\n\\t\\...   \n",
       "10  \\n\\n      АГОНИЯ   МАСОНОВ\\n\\nДа вы поняли хот...   \n",
       "11  \\n\\nФрагмент. Книгу можно купить в таких магаз...   \n",
       "12  \\n\\nФрагмент. Книгу можно купить в таких магаз...   \n",
       "13  \\n\\n\\nОгненное солнце, зажженное демоном под к...   \n",
       "14  \\n \\n              Кардинал Луи Де Шеврон поль...   \n",
       "\n",
       "                                               lemmas  \n",
       "0   что-то стучаться висок кровь, недавний выстрел...  \n",
       "1   макс роуд љ 05.11.2014 - 03.04.2015 агония маз...  \n",
       "2   ivan mak идеальный мир предисловие медленно по...  \n",
       "3   глава 15 *** 4403 цикл космический эра планет ...  \n",
       "4   малиновкий майя пленник уэст книга 2 фантастич...  \n",
       "5   ленточка фанфик роман андрей круз \"земля лишни...  \n",
       "6   \" mass effect : while the reaper \" назва: \" ma...  \n",
       "7   дмитрий крам s-t-i-k-s подкидыш улья андрей пр...  \n",
       "8   дмитрий крам исчезнуть клан фанфик \"играть жит...  \n",
       "9   содержание: часть 1 акклиматиазия часть 2 полё...  \n",
       "10  агония масон понять хоть, понатворить одессе, ...  \n",
       "11  фрагмент книга купить такой магазинах, 'литрес...  \n",
       "12  фрагмент книга купить такой магазинах, 'литрес...  \n",
       "13  огненный солнце, зажечь демон купол пещеры, ме...  \n",
       "14  кардинал луи де шеврон пользоваться среди горо...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words векторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['lemmas'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yv__Q1Wb1ny"
   },
   "source": [
    "## [6 баллов] Задача о тематическом моделировании\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-D75Q_5HF9"
   },
   "source": [
    "### об LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoZTPslmm0d9"
   },
   "source": [
    "\n",
    "\n",
    "### Постановка задачи\n",
    "Загрузите [коллекцию писем Х. Клинтон](https://www.kaggle.com/kaggle/hillary-clinton-emails/?select=Emails.csv) с kaggle. Для скачивания может потребоваться регистрация.\n",
    "\n",
    "Методом LDA выделите несколько тем в переписке Х. Клинтон, дайте им словесное описание. Используйте библиотеку LdaModel из gensim.\n",
    "\n",
    "#### Критерии\n",
    "\n",
    "* **2 БАЛЛА**: получены списки ключевых слов, не выглядящие бессмыслицей\n",
    "* **2 БАЛЛА**: осуществлена визуализация библиотекой pyLDAvis\n",
    "* **1 БАЛЛ**: предложено осмысленное текстовое описание большинства выделенных тем.\n",
    "* **1 БАЛЛ**: проведено сравнение LDA, запущенного на CountVectorizer и TfIdfVectorizer предтавлениях одних и тех же данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqnaLMAVm0Sb"
   },
   "source": [
    "#### примеры кода\n",
    "\n",
    "Пример обучения LdaModel на выдаче CountVectorizer: [ссылка](https://github.com/EricSchles/sklearn_gensim_example/blob/master/example.py)\n",
    "\n",
    "Пример использования pyLDAvis: секция 15 [по ссылке](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)\n",
    "\n",
    "---\n",
    "\n",
    "Для обучения *LdaModel* и её последующей визуализации потребуется словарь формата gensim. Словарь формата gensim удобно получать из сжатого csc_matrix-представления нашего векторизованного текста: как многие замечали на паре, tf-idf векторы содержат много нулей.\n",
    "\n",
    "```python\n",
    "import gensim\n",
    "from scipy.sparse import csc\n",
    "\n",
    "corpus = gensim.matutils.Sparse2Corpus(csc.csc_matrix(X))\n",
    "dictionary = gensim.corpora.Dictionary.from_corpus(corpus, vocab_dict)\n",
    "```\n",
    "\n",
    "где *corpora* содержит полученное с помощью gensim представление коллекции, а *vocab_dict* — это dict, полученный после работы Vectorizer, ставящий в соответствие каждому номеру строки в матрице данных само слово в виде строки."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPlA8mm5agWQ4XIne5tjyqH",
   "include_colab_link": true,
   "name": "домашнее задание о векторизации текстов.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
